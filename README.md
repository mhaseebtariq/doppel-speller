# DoppelSpeller

Finds the best match (in a database of titles) for a misspelled title,
using a combination of Machine Learning and NLP techniques.<br/><br/>
![Project description](./description.jpg)

## Setup
* **Pre-requisites**:
    - Install [Docker](https://docs.docker.com/install/) (tested on engine v3.7)
    - Install `make`:
        - Windows: Install [Cygwin](https://www.cygwin.com/setup-x86_64.exe) [while on the screen that lets you select packages to install, find `make` and select it]
        - Debian: `apt-get install build-essential`
        - RHEL: `yum install make`
        - macOS: Xcode `xcode-select --install` | or using Homebrew `brew install make`
* Check [cli.py](./doppelspeller/cli.py) and [Makefile](./Makefile) for cli definitions
* `make --always-make build` - to build and prepare the Docket container for running the project
* `make update-docker` - to update the project setup on the Docker container
* `make stage-example-data-set` - to copy the "example" data set files to the Docker container
* `make inspect` - inspect the code for PEP-8 issues
* `make test` - run the unit tests

## Explanation

Run the following cli's in order:

#### `make generate-lsh-forest`
Alias of `generate_lsh_forest` in [cli.py](./doppelspeller/cli.py)
* Given the "truth" database (see `GROUND_TRUTH_FILE` in [settings.py](./doppelspeller/settings.py)):
    - Generates a Locality-sensitive hashing (LSH) forest for fetching the nearest title matches
    - "Nearest", based on the Jaccard distance computed on ngrams (n=3) of the titles
* The computation can definitely be improved by using a different distance metric, computed over high dimensional (character <-> numeric encoded) matrices

#### `make prepare-data-for-features-generation`
Alias of `prepare_data_for_features_generations` in [cli.py](./doppelspeller/cli.py)
* Prepares training data for a `OneVsRestClassifier`
* Each "positive" match is trained along with the nearest "n" matches that do not match with the title

#### `make generate-train-and-evaluation-data-sets`
Alias of `generate_train_and_evaluation_data_sets` in [cli.py](./doppelspeller/cli.py)
* Generates `train` and `evaluation` data sets for the `train-model` cli

#### `make train-model`
Alias of `train_model` in [cli.py](./doppelspeller/cli.py)
* XGBoost training output: `train-auc:1	evaluation-auc:0.999893	train-custom-error:5	evaluation-custom-error:178`
* See the definition of `custom_error` in [train.py](./doppelspeller/train.py)
    - Also, the custom objective function `weighted_log_loss`

#### `make prepare-predictions-data`
Alias of `prepare_predictions_data` in [cli.py](./doppelspeller/cli.py)
* Fetches and saves the nearest title matches per "prediction" row, using the LSH forest generated by the `generate-lsh-forest` cli

#### `make generate-predictions`
Alias of `generate_predictions` in [cli.py](./doppelspeller/cli.py)
* The algorithm first looks for exact matches
* Then the nearest "n" matches per (remaining) title are found using the LSH forest
* Next, the nearest matches are "fuzzy" matched with each title
* Finally, the trained model is used to match the remaining titles
* Test set predictions accuracy (run `make get-predictions-accuracy` to calculate the following):
```
True Positives      5877
True Negatives      3863
False Positives     123
False Negatives     137
```

#### `make extensive-search-single-title title='PRO teome plc SCIs'`
Alias of `extensive_search_single_title` in [cli.py](./doppelspeller/cli.py)
* Predicts the best match using the `OneVsRestClassifier` for the entire (not just the nearest matches) "truth" database

## NOTES
* All the computationally expensive tasks run in multi-processing mode
* Those tasks, can therefore, be easily refactored to run on distributed computing clusters

## TODO
* Extend README to include more details/explanation of the solution
* Document all the classes/methods in the code
* Write more unit tests
* Refactor code to be more SOLID
